{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mcpenguin/spaceship-titanic-eda-and-prediction?scriptVersionId=143235257\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, cross_val_score\nimport xgboost as xgb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-11T02:22:19.296765Z","iopub.execute_input":"2023-08-11T02:22:19.297207Z","iopub.status.idle":"2023-08-11T02:22:19.304204Z","shell.execute_reply.started":"2023-08-11T02:22:19.29717Z","shell.execute_reply":"2023-08-11T02:22:19.30278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. Load Datasets","metadata":{}},{"cell_type":"code","source":"INPUT_DIR = \"../input/spaceship-titanic\"\ny_variate = \"Transported\"\n\ntrain_data = pd.read_csv(f\"{INPUT_DIR}/train.csv\")\ntest_data = pd.read_csv(f\"{INPUT_DIR}/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:19.311Z","iopub.execute_input":"2023-08-11T02:22:19.312747Z","iopub.status.idle":"2023-08-11T02:22:19.384024Z","shell.execute_reply.started":"2023-08-11T02:22:19.312673Z","shell.execute_reply":"2023-08-11T02:22:19.382937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. First Look at Training and Testing Data","metadata":{}},{"cell_type":"markdown","source":"Let's see what the training and testing datasets look like:","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:19.387339Z","iopub.execute_input":"2023-08-11T02:22:19.38824Z","iopub.status.idle":"2023-08-11T02:22:19.414437Z","shell.execute_reply.started":"2023-08-11T02:22:19.38819Z","shell.execute_reply":"2023-08-11T02:22:19.41295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:19.416413Z","iopub.execute_input":"2023-08-11T02:22:19.416943Z","iopub.status.idle":"2023-08-11T02:22:19.443079Z","shell.execute_reply.started":"2023-08-11T02:22:19.416888Z","shell.execute_reply":"2023-08-11T02:22:19.441793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initial observations:\n\n* According to the problem brief, the `Cabin` variate actually consists of three variates: the deck, the number and the side. We will want to extract these before doing any more data analyses.\n\n* Moreover, the `PassengerId` variate actually consists of two components: the group a passenger is travelling with, and their number within the group. We will also want to extract these before doing any more data analyses.\n\nSimilarly, we can check their dimensions:","metadata":{}},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:19.448539Z","iopub.execute_input":"2023-08-11T02:22:19.44905Z","iopub.status.idle":"2023-08-11T02:22:19.457595Z","shell.execute_reply.started":"2023-08-11T02:22:19.449006Z","shell.execute_reply":"2023-08-11T02:22:19.456294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:19.458992Z","iopub.execute_input":"2023-08-11T02:22:19.45932Z","iopub.status.idle":"2023-08-11T02:22:19.473384Z","shell.execute_reply.started":"2023-08-11T02:22:19.459291Z","shell.execute_reply":"2023-08-11T02:22:19.472116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's check whether there are any missing values in the dataset:","metadata":{}},{"cell_type":"code","source":"train_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:19.475584Z","iopub.execute_input":"2023-08-11T02:22:19.47616Z","iopub.status.idle":"2023-08-11T02:22:19.514998Z","shell.execute_reply.started":"2023-08-11T02:22:19.476126Z","shell.execute_reply":"2023-08-11T02:22:19.51392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Before we proceed with our analyses, it might be helpful to set the missing variates explicitly in our data, as well as extract some of the useful aforementioned features to our dataset.","metadata":{}},{"cell_type":"code","source":"# function to process data\ndef process_data(data):\n    # split cabin into three columns\n    deck_split = pd.DataFrame(data[\"Cabin\"].str.split(\"/\").apply(pd.Series))\n    data[[\"Deck\", \"Num\", \"Side\"]] = pd.DataFrame(deck_split)\n    data.drop([\"Cabin\"], axis=1)\n    data[\"Num\"] = pd.to_numeric(data['Num'])\n    \n    passengerId_split = pd.DataFrame(data[\"PassengerId\"].str.split(\"_\").apply(pd.Series))\n    data[[\"Group\", \"NumberInGroup\"]] = pd.DataFrame(passengerId_split)\n    \n    # replace nans in columns with appropriate values\n    data[\"HomePlanet\"] = data[\"HomePlanet\"].replace(np.nan, \"Missing\")\n    data[\"CryoSleep\"] = data[\"CryoSleep\"].replace(np.nan, False)\n    data[\"Destination\"] = data[\"Destination\"].replace(np.nan, \"Missing\")\n    data[\"VIP\"] = data[\"VIP\"].replace(np.nan, False)\n    data[[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]] = data[[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]].replace(np.nan, 0)\n    data[\"Age\"] = data[\"Age\"].replace(np.nan, 0)\n    \n    # replace missing values for deck, num, side\n    data[\"Deck\"] = data[\"Deck\"].replace(np.nan, 'M')\n    data[\"Num\"] = data[\"Num\"].replace(np.nan, 0)\n    data[\"Side\"] = data[\"Side\"].replace(np.nan, 'M')\n    \n    return data\n\ntrain_data = process_data(train_data)\ntest_data = process_data(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:19.517029Z","iopub.execute_input":"2023-08-11T02:22:19.517674Z","iopub.status.idle":"2023-08-11T02:22:24.006976Z","shell.execute_reply.started":"2023-08-11T02:22:19.51761Z","shell.execute_reply":"2023-08-11T02:22:24.005764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. EDA\n\n## 3.1. Home Planet\n\nNext, we can perform some exploratory data analysis on the dataset. We can first start by examining the distribution of `Transported` by `HomePlanet` across both the training and test data. In particular, for the training dataset, we can investigate the distribution of the variate across passengers who were transported compared to those who were not:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2)\nfig.suptitle(\"Transported\")\norder = [\"Earth\", \"Europa\", \"Mars\", \"Missing\"]\n\nsns.countplot(ax=axes[0], data=train_data, x=\"HomePlanet\", hue=\"Transported\", order=order)\naxes[0].set_title(\"Training Data\")\nsns.countplot(ax=axes[1], data=test_data, x=\"HomePlanet\", order=order)\naxes[1].set_title(\"Test Data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:24.008493Z","iopub.execute_input":"2023-08-11T02:22:24.009098Z","iopub.status.idle":"2023-08-11T02:22:24.515392Z","shell.execute_reply.started":"2023-08-11T02:22:24.009064Z","shell.execute_reply":"2023-08-11T02:22:24.514228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that for both datasets, a majority of the population come from `Earth`, with a roughly similar proportion of the population coming from `Europa` and `Mars`. We see a similar distribution between passengers who were transported vs. those who were not.\n\n## 3.2. Groups\n\nWe can also investigate how many groups there are in the training dataset:","metadata":{}},{"cell_type":"code","source":"print(\"Number of groups in training dataset:\", len(train_data[\"Group\"].unique()))\nprint(\"Total number of data in training dataset:\", train_data.shape[0])\n\nprint(\"Number of groups in test dataset:\", len(test_data[\"Group\"].unique()))\nprint(\"Total number of data in test dataset:\", test_data.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:24.519667Z","iopub.execute_input":"2023-08-11T02:22:24.520505Z","iopub.status.idle":"2023-08-11T02:22:24.531016Z","shell.execute_reply.started":"2023-08-11T02:22:24.520454Z","shell.execute_reply":"2023-08-11T02:22:24.529831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the number of groups is very close to the number of data points in both datasets, it suggests that each group is nearly unique to each passenger. Therefore, it might not make sense to put this in our model.\n\n## 3.3. Destination\n\nWe can also investigate the `Destination` variate for both datasets. In particular, for the plot with the training dataset, we can investigate the count between passengers who were transported compared to those who were not:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\nfig.suptitle(\"Destination\")\norder = ['TRAPPIST-1e', 'PSO J318.5-22', '55 Cancri e', 'Missing']\n\nsns.countplot(ax=axes[0], data=train_data, x=\"Destination\", hue=\"Transported\", order=order)\naxes[0].set_title(\"Training Data\")\nsns.countplot(ax=axes[1], data=test_data, x=\"Destination\", order=order)\naxes[1].set_title(\"Test Data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:24.532547Z","iopub.execute_input":"2023-08-11T02:22:24.53397Z","iopub.status.idle":"2023-08-11T02:22:25.120801Z","shell.execute_reply.started":"2023-08-11T02:22:24.533916Z","shell.execute_reply":"2023-08-11T02:22:25.119512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We again see a similar distribution between `Destination` for both datasets, and a similar distribution between passengers who were transported and who were not in the training dataset.\n\n## 3.4. Boolean Variates\n\nSubsequently, we can investigate the distribution of the boolean variates:","metadata":{}},{"cell_type":"code","source":"for variate in ['CryoSleep', 'VIP']:\n    print(f\"Proportion of training data with true {variate}: {train_data[variate].sum()} / {train_data.shape[0]}\")\n    print(f\"Proportion of test data with true {variate}: {test_data[variate].sum()} / {test_data.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:25.122411Z","iopub.execute_input":"2023-08-11T02:22:25.122937Z","iopub.status.idle":"2023-08-11T02:22:25.130988Z","shell.execute_reply.started":"2023-08-11T02:22:25.1229Z","shell.execute_reply":"2023-08-11T02:22:25.129886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that about one third of the passengers have `CryoSleep`, whereas about 5% of both the training and test datasets have `VIP`, which aligns with our intuitions.\n\n## 3.5. Cabin Variates\n\nNext, we can then analyze the `Deck`, `Num` and `Side` in the data. It seems like `Num` has many unique values, which makes sense intuitively, so let us only focus on the other two variates.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfig.tight_layout(pad=5.0)\ncabin_variates = ['Deck', 'Side']\n\nfor idx, v in enumerate(cabin_variates):\n    sns.countplot(ax=axes[idx, 0], data=train_data, x=v, hue=\"Transported\")\n    axes[idx, 0].set_title(f\"Training Data: {v}\")\n    sns.countplot(ax=axes[idx, 1], data=test_data, x=v)\n    axes[idx, 1].set_title(f\"Test Data: {v}\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:25.132512Z","iopub.execute_input":"2023-08-11T02:22:25.133626Z","iopub.status.idle":"2023-08-11T02:22:26.4879Z","shell.execute_reply.started":"2023-08-11T02:22:25.133584Z","shell.execute_reply":"2023-08-11T02:22:26.485757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this, it seems like the distribution of passengers who got transported across the `Deck` and `Side` variates are similar.","metadata":{}},{"cell_type":"markdown","source":"## 3.6. Luxury Amenities Variates\n\nNext, we can investigate the variates associated with the luxury amenities. For the training data, we can further categorize the data by whether the passenger was Transported or not:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(15, 15))\nfig.tight_layout(pad=5.0)\nluxury_variates = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\nfor idx, v in enumerate(luxury_variates):\n    sns.histplot(ax=axes[idx, 0], data=train_data, x=v, hue=\"Transported\", bins=30)\n    axes[idx, 0].set_title(f\"Training Data: {v}\")\n    sns.histplot(ax=axes[idx, 1], data=test_data, x=v, bins=30)\n    axes[idx, 1].set_title(f\"Test Data: {v}\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:26.4898Z","iopub.execute_input":"2023-08-11T02:22:26.49052Z","iopub.status.idle":"2023-08-11T02:22:31.247341Z","shell.execute_reply.started":"2023-08-11T02:22:26.490443Z","shell.execute_reply":"2023-08-11T02:22:31.246089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All these histograms are very skewed to the left, so using log transformations on all of these might help stabilize their variance and make the histograms more symmetric:","metadata":{}},{"cell_type":"code","source":"for data in train_data, test_data:\n    for v in luxury_variates:\n        data[\"Transformed\" + v] = data[v].apply(lambda x: np.log(x+1))\n    data.drop(luxury_variates, axis=1)\n\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(15, 15))\nfig.tight_layout(pad=5.0)\n\nfor idx, v in enumerate(map(lambda x: \"Transformed\" + x, luxury_variates)):\n    sns.histplot(ax=axes[idx, 0], data=train_data, x=v, hue=\"Transported\", bins=30)\n    axes[idx, 0].set_title(f\"Training Data: {v}\")\n    sns.histplot(ax=axes[idx, 1], data=test_data, x=v, bins=10)\n    axes[idx, 1].set_title(f\"Test Data: {v}\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:31.249011Z","iopub.execute_input":"2023-08-11T02:22:31.249489Z","iopub.status.idle":"2023-08-11T02:22:35.089068Z","shell.execute_reply.started":"2023-08-11T02:22:31.249423Z","shell.execute_reply":"2023-08-11T02:22:35.087821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the histograms are a lot more symmetric now. In particular, we see that the distributions of the luxury variates of passengers who were transported is very similar to the distribution of those who were not transported.","metadata":{}},{"cell_type":"markdown","source":"# 3. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Before modelling, we need to convert all our categorical variates into one-hot encodings.","metadata":{}},{"cell_type":"code","source":"def make_one_hot_encodings(data):\n    categorical_cols = [\"HomePlanet\", \"Deck\", \"Side\", \"Destination\"]\n    data = pd.get_dummies(data, columns=categorical_cols) \n    return data\n    \ntrain_data = make_one_hot_encodings(train_data)\ntest_data = make_one_hot_encodings(test_data)\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:35.090936Z","iopub.execute_input":"2023-08-11T02:22:35.091747Z","iopub.status.idle":"2023-08-11T02:22:35.154182Z","shell.execute_reply.started":"2023-08-11T02:22:35.091705Z","shell.execute_reply":"2023-08-11T02:22:35.153321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:22:35.155512Z","iopub.execute_input":"2023-08-11T02:22:35.155904Z","iopub.status.idle":"2023-08-11T02:22:35.164228Z","shell.execute_reply.started":"2023-08-11T02:22:35.155873Z","shell.execute_reply":"2023-08-11T02:22:35.162769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Modelling\n\nWe are now ready to start modelling. Our strategy will be to use `GradientBoostingRegression` with k-fold cross validation.\n\nWe first need to prepare our k-fold samples by removing any variates not needed in the calculation and splitting the data up correspondingly.","metadata":{}},{"cell_type":"code","source":"k = 5\nkf = KFold(n_splits=k)\n\ndef prepare_Xy(data, isTest=False):\n    X = data.drop([\"PassengerId\", \"Name\", \"Group\", \"NumberInGroup\", \"Cabin\"], axis=1)\n    if isTest:\n        return X\n    X = X.drop([\"Transported\"], axis=1)\n    y = data[\"Transported\"]\n    return X, y\n\nX_train, y_train = prepare_Xy(train_data)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:24:50.797627Z","iopub.execute_input":"2023-08-11T02:24:50.798022Z","iopub.status.idle":"2023-08-11T02:24:50.83638Z","shell.execute_reply.started":"2023-08-11T02:24:50.797992Z","shell.execute_reply":"2023-08-11T02:24:50.83499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now fit the model and calculate the average cross-validation score:","metadata":{}},{"cell_type":"code","source":"xgbc = xgb.XGBClassifier(n_estimators=100)\nxgbc.fit(X_train, y_train)\n\nkfold = KFold(n_splits=10, shuffle=True)\nkf_cv_scores = cross_val_score(xgbc, X_train, y_train, cv=kfold)\nprint(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:24:53.07122Z","iopub.execute_input":"2023-08-11T02:24:53.071664Z","iopub.status.idle":"2023-08-11T02:25:06.299917Z","shell.execute_reply.started":"2023-08-11T02:24:53.071625Z","shell.execute_reply":"2023-08-11T02:25:06.299004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Prediction\n\nFinally, we can predict the `Transported` variate for the test values.","metadata":{}},{"cell_type":"code","source":"X_test = prepare_Xy(test_data, isTest=True)\ny_test = xgbc.predict(X_test)\n\nsubmission = test_data[[\"PassengerId\"]]\nsubmission[\"Transported\"] = y_test.astype(bool)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:25:34.822707Z","iopub.execute_input":"2023-08-11T02:25:34.823134Z","iopub.status.idle":"2023-08-11T02:25:34.916441Z","shell.execute_reply.started":"2023-08-11T02:25:34.823102Z","shell.execute_reply":"2023-08-11T02:25:34.915154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit submission file to competition\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T02:25:38.485115Z","iopub.execute_input":"2023-08-11T02:25:38.485531Z","iopub.status.idle":"2023-08-11T02:25:38.505384Z","shell.execute_reply.started":"2023-08-11T02:25:38.485498Z","shell.execute_reply":"2023-08-11T02:25:38.504051Z"},"trusted":true},"execution_count":null,"outputs":[]}]}