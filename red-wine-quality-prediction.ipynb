{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mcpenguin/red-wine-quality-prediction?scriptVersionId=143235380\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Red Wine Classification\n\nIn this notebook, we investigate the red wine dataset and build a model to\npredict their relative quality.","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"# basic libraries / data processing\nimport numpy as np\nimport pandas as pd\nimport os\n\n# graphical visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# preprocessing\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# modelling\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-29T03:53:47.940839Z","iopub.execute_input":"2023-08-29T03:53:47.941349Z","iopub.status.idle":"2023-08-29T03:53:47.951299Z","shell.execute_reply.started":"2023-08-29T03:53:47.941293Z","shell.execute_reply":"2023-08-29T03:53:47.948663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Datasets","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T02:50:19.945574Z","iopub.execute_input":"2023-08-29T02:50:19.946053Z","iopub.status.idle":"2023-08-29T02:50:20.008975Z","shell.execute_reply.started":"2023-08-29T02:50:19.946014Z","shell.execute_reply":"2023-08-29T02:50:20.006759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try to get initial information about this dataset by calling `info` and `describe`:","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T02:50:23.986925Z","iopub.execute_input":"2023-08-29T02:50:23.987334Z","iopub.status.idle":"2023-08-29T02:50:24.017331Z","shell.execute_reply.started":"2023-08-29T02:50:23.98729Z","shell.execute_reply":"2023-08-29T02:50:24.016082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T02:50:34.509371Z","iopub.execute_input":"2023-08-29T02:50:34.511037Z","iopub.status.idle":"2023-08-29T02:50:34.55802Z","shell.execute_reply.started":"2023-08-29T02:50:34.510965Z","shell.execute_reply":"2023-08-29T02:50:34.55656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these, we see that there are no missing values.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Histograms of Variates\n\nTo better understand the distributions of the variates in the data, we can plot histograms for the individual variates.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(16, 9))\nplt.subplots_adjust(hspace=1)\n\nfor ax, col in zip(axes.flat, df.columns):\n    sns.histplot(data=df, x=col, ax=ax)\n    ax.set_title(f\"Distribution of {col}\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:01:26.550867Z","iopub.execute_input":"2023-08-29T03:01:26.55127Z","iopub.status.idle":"2023-08-29T03:01:30.953274Z","shell.execute_reply.started":"2023-08-29T03:01:26.551245Z","shell.execute_reply":"2023-08-29T03:01:30.951732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that wine quality is actually a categorical (integer) value. As such, it makes sense to treat it as a category during our EDA.","metadata":{}},{"cell_type":"markdown","source":"## Boxenplots of Variates vs. Quality\n\nAs the quality of the wine is the thing that we want to predict, a natural graphical visualization we could look at would be distributions of the variates after taking the quality of the wine into account.","metadata":{}},{"cell_type":"code","source":"# convert quality to categorical variate\ndf[\"quality category\"] = df[\"quality\"].astype(\"category\")\n\nfig, axes = plt.subplots(nrows=11, ncols=1, figsize=(16, 100))\n# plt.subplots_adjust(hspace=1)\nfor ax, col in zip(axes.flat, df.columns[:-1]):\n    sns.boxenplot(data=df, y=col, x=\"quality\", ax=ax)\n    ax.set_title(f\"Distribution of {col} vs. Quality\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:12:06.936237Z","iopub.execute_input":"2023-08-29T03:12:06.936729Z","iopub.status.idle":"2023-08-29T03:12:09.981755Z","shell.execute_reply.started":"2023-08-29T03:12:06.936689Z","shell.execute_reply":"2023-08-29T03:12:09.980887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Matrix\n\nWe can better understand the correlations between the variates by using a correlation matrix.","metadata":{}},{"cell_type":"code","source":"ax = sns.heatmap(df.corr(numeric_only=True))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:02:34.63205Z","iopub.execute_input":"2023-08-29T03:02:34.632652Z","iopub.status.idle":"2023-08-29T03:02:35.000106Z","shell.execute_reply.started":"2023-08-29T03:02:34.632612Z","shell.execute_reply":"2023-08-29T03:02:34.998186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this, we see that the variates are relatively uncorrelated. The only contenders of multicollinearity seem to be \n\n* `fixed acidity` and `citric acid`;\n\n* `free sulfur dioxide` and `total sulfur dioxide`.\n\nWe might want to consider removing these variates before modelling to improve model performance.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Sulfur Dioxide Ratio\n\nGiven the trends that we observed in our EDA, a natural variate we could use in place of the sulfur dioxide variates would be the ratio of free vs. total sulfur dioxide in the wine.","metadata":{}},{"cell_type":"code","source":"df[\"sulfur dioxide ratio\"] = df[\"free sulfur dioxide\"] / df[\"total sulfur dioxide\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:13:16.509768Z","iopub.execute_input":"2023-08-29T03:13:16.510153Z","iopub.status.idle":"2023-08-29T03:13:16.521288Z","shell.execute_reply.started":"2023-08-29T03:13:16.510119Z","shell.execute_reply":"2023-08-29T03:13:16.518256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the distribution of this transformed variate in our data:","metadata":{}},{"cell_type":"code","source":"ax = sns.histplot(data=df, x=\"sulfur dioxide ratio\")\nax.set_title(f\"Distribution of sulfur dioxide ratio\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:14:38.736889Z","iopub.execute_input":"2023-08-29T03:14:38.737284Z","iopub.status.idle":"2023-08-29T03:14:38.979169Z","shell.execute_reply.started":"2023-08-29T03:14:38.73725Z","shell.execute_reply":"2023-08-29T03:14:38.978255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Additionally, we can see the distributions of sulfur dioxide after taking into account the quality of the wine:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nax = sns.boxenplot(data=df, y=\"sulfur dioxide ratio\", x=\"quality category\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:16:12.041169Z","iopub.execute_input":"2023-08-29T03:16:12.041657Z","iopub.status.idle":"2023-08-29T03:16:12.286231Z","shell.execute_reply.started":"2023-08-29T03:16:12.041618Z","shell.execute_reply":"2023-08-29T03:16:12.284299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train/Test Split\n\nPrior to modelling, we will split our data into training, validation and test sets.","metadata":{}},{"cell_type":"code","source":"explanatory_cols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'sulfur dioxide ratio', 'density', 'pH', 'sulphates', 'alcohol']\nresponse_col = ['quality']\n\nX = df[explanatory_cols]\ny = df[response_col]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\nprint(f\"Shape of y_test: {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-29T04:21:29.10438Z","iopub.execute_input":"2023-08-29T04:21:29.104749Z","iopub.status.idle":"2023-08-29T04:21:29.118684Z","shell.execute_reply.started":"2023-08-29T04:21:29.104722Z","shell.execute_reply":"2023-08-29T04:21:29.116908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n\nFor our predictive model, we will be trying different regression algorithms and choosing the best one to tune. To do this, we will be employing (stratified) K-fold cross-validation.","metadata":{}},{"cell_type":"code","source":"models = [\n    ('random_forest', RandomForestRegressor()),\n    ('gradient_boosting', GradientBoostingRegressor()),\n    ('hist_gradient_boosting', HistGradientBoostingRegressor()),\n    ('svr', SVR()),\n    ('k_neighbors', KNeighborsRegressor())\n]","metadata":{"execution":{"iopub.status.busy":"2023-08-29T04:10:39.870952Z","iopub.execute_input":"2023-08-29T04:10:39.871566Z","iopub.status.idle":"2023-08-29T04:10:39.877164Z","shell.execute_reply.started":"2023-08-29T04:10:39.87153Z","shell.execute_reply":"2023-08-29T04:10:39.87614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def create_pipeline(name, model):\n    return Pipeline([\n        ('scaler', StandardScaler()),\n        (name, model)\n    ])\n\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle=True)\n\nbest_loss = np.inf\nbest_model_name = None\n\nfor name, model in models:\n    print(\"-\"*25)\n    print(f\"Training MODEL {name}\")\n    print(\"-\"*25)\n    val_losses = []\n    \n    for i, (train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n        print(f\"Training on FOLD {i+1}\")\n        X_train_act = X_train.iloc[train_index].values\n        X_val = X_train.iloc[val_index].values\n        y_train_act = y_train.iloc[train_index].values.ravel()\n        y_val = y_train.iloc[val_index].values.ravel()\n        \n        pipe = create_pipeline(name, model)\n        \n        pipe.fit(X_train_act, y_train_act)\n        y_val_pred = pipe.predict(X_val)\n        mse = mean_squared_error(y_val, y_val_pred)\n        print(f\"MSE on validation set: {mse}\")\n        print()\n        \n        val_losses.append(mse)\n    \n    average_loss = np.mean(val_losses)\n    print(f\"Average MSE: {average_loss}\")\n    if average_loss < best_loss:\n        best_loss = average_loss\n        best_model_name = name\n\nprint()\nprint(f\"Best model: {best_model_name}\")\nprint(f\"MSE: {best_loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-29T04:13:16.293925Z","iopub.execute_input":"2023-08-29T04:13:16.294388Z","iopub.status.idle":"2023-08-29T04:13:22.382322Z","shell.execute_reply.started":"2023-08-29T04:13:16.294355Z","shell.execute_reply":"2023-08-29T04:13:22.381221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n\nWe can further tune the parameters of the `RandomForest` model using `GridSearchCV` to try to get a better score.","metadata":{}},{"cell_type":"code","source":"models_dict = dict(models)\nbest_model = models_dict[best_model_name]\n\npipeline = create_pipeline(best_model_name, best_model)\nparameters_grid = {\n    f\"random_forest__n_estimators\": [30, 50, 100, 200],\n    f\"random_forest__max_depth\": [None, 4, 6],\n    f\"random_forest__max_features\": [None, \"sqrt\", \"log2\"]\n}\nsearch = GridSearchCV(pipeline, parameters_grid)\n\nsearch.fit(X_train, y_train.values.ravel())","metadata":{"execution":{"iopub.status.busy":"2023-08-29T04:17:33.526199Z","iopub.execute_input":"2023-08-29T04:17:33.527623Z","iopub.status.idle":"2023-08-29T04:18:23.45989Z","shell.execute_reply.started":"2023-08-29T04:17:33.527541Z","shell.execute_reply":"2023-08-29T04:18:23.45774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the best parameters:","metadata":{}},{"cell_type":"code","source":"search.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-08-29T04:18:27.972103Z","iopub.execute_input":"2023-08-29T04:18:27.972727Z","iopub.status.idle":"2023-08-29T04:18:27.984913Z","shell.execute_reply.started":"2023-08-29T04:18:27.972683Z","shell.execute_reply":"2023-08-29T04:18:27.982502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance\n\nWe can next investigate the relative feature importance of our model.","metadata":{}},{"cell_type":"code","source":"best_pipeline = search.best_estimator_\nfeature_importances = best_pipeline.named_steps[\"random_forest\"].feature_importances_\n\nplt.figure(figsize=(12, 8))\nplt.xticks(rotation = 60)\n\nax = sns.barplot(x=explanatory_cols, y=feature_importances)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T04:19:11.794856Z","iopub.execute_input":"2023-08-29T04:19:11.795316Z","iopub.status.idle":"2023-08-29T04:19:12.070065Z","shell.execute_reply.started":"2023-08-29T04:19:11.795275Z","shell.execute_reply":"2023-08-29T04:19:12.067744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n\nWe can now predict the wine quality scores on the test dataset.","metadata":{}},{"cell_type":"code","source":"y_pred = best_pipeline.predict(X_test)\nprint(f\"MSE for test dataset: {mean_squared_error(y_pred, y_test)}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-29T04:21:37.379534Z","iopub.execute_input":"2023-08-29T04:21:37.380044Z","iopub.status.idle":"2023-08-29T04:21:37.42011Z","shell.execute_reply.started":"2023-08-29T04:21:37.380006Z","shell.execute_reply":"2023-08-29T04:21:37.417384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see the accuracy of the predictions, we can plot the predictions and the actual ratings for the test dataset on a scatterplot. If our predictive model is good, we should expect to see that the points should be close to the line `y = x`.","metadata":{}},{"cell_type":"code","source":"flat_y_test = y_test.values.flatten()\nax = sns.scatterplot(x=y_pred, y=flat_y_test)\nsns.lineplot(x=flat_y_test, y=flat_y_test, ax=ax)\nax.set_xlabel(\"Predicted Rating\")\nax.set_ylabel(\"Actual Rating\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T04:28:12.68025Z","iopub.execute_input":"2023-08-29T04:28:12.682092Z","iopub.status.idle":"2023-08-29T04:28:13.000598Z","shell.execute_reply.started":"2023-08-29T04:28:12.682034Z","shell.execute_reply":"2023-08-29T04:28:12.998072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that our model does a pretty good job at predicting the wine rating.","metadata":{}},{"cell_type":"markdown","source":"# Thanks for reading!","metadata":{}}]}